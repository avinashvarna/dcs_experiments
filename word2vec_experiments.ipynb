{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Experiments using DCS\n",
    "\n",
    "For a [Sanskrit parser project](https://github.com/kmadathil/sanskrit_parser) that I am collaborating on, we have been discussing and investigating different language models, and their applicability to parsing Sanskrit. I have been particularly interested in deep learning approaches for language modeling, such as Seq2Seq (+ attention), etc. A building block in many of these deep learning approaches is the embedding of words in a vector space using word2vec or GloVe. This notebook contains some of my experiments with word2vec using the Digital Corpus of Sanskrit to investigate the feasibility of using word2vec on just root words (prAtipadikas/dhAtus) in Sanskrit.\n",
    "\n",
    "The DCS database is quite small from a deep learning perspective (about 30 MB if we count just the root words), so it was unclear how good the results would be or what to expect. (Spoiler - I was pleasantly surprised by the quality of the results obtained for a first pass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALVARNA\\programs\\anaconda\\lib\\site-packages\\gensim-2.3.0-py2.7-win-amd64.egg\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import gensim\n",
    "import logging\n",
    "import codecs\n",
    "import zipfile\n",
    "import os\n",
    "import itertools\n",
    "import pprint\n",
    "from indic_transliteration import sanscript\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "For this experiment, let us investigate the feasibility of using just the root words (ignoring the POS tags, which in Sanskrit would be vibhakti/vachana/puruSha/lakAra) for embedding. The DCS data has been preprocessed and saved as a csv file of the form:\n",
    "id, sentence, roots\n",
    "based on the annotations in the DCS database. Note that the database refers to half a shloka as a sentence in most cases, so this might not be a full sentence / vAkya.\n",
    "\n",
    "The file sent_roots.zip in the git repository has a zipped form of this file, which the next cell will unzip if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from zip file ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data_file = \"sent_roots.csv\"\n",
    "if not os.path.exists(data_file):\n",
    "    print(\"Extracting from zip file ...\")\n",
    "    with zipfile.ZipFile(os.path.splitext(data_file)[0] + \".zip\", 'r') as myzip:\n",
    "        myzip.extract(data_file)\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(data_file, \"already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec only requires an iterator that yields one sentence at a time as a list of words as described in this [tutorial](https://rare-technologies.com/word2vec-tutorial/). Let us adapt the class given as an example there to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCSSentences(object):\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    " \n",
    "    def __iter__(self):\n",
    "        with codecs.open(self.data_file, 'rb', \"utf8\") as f:\n",
    "            for line in f:\n",
    "                yield line.split(\",\")[-1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'paYcan', u'ratna', u'muKya', u'ca', u'uparatna', u'catuzwaya'],\n",
      " [u'pravAla', u'lohita', u'pravac', u'vEqUrya', u'harita', u'pARqura'],\n",
      " [u'ABIra', u'pAnTa', u'taTA', u'api', u'ca', u'vanya'],\n",
      " [u'jYA', u'maDUla', u'saMjYA', u'api', u'maDUka', u'vArisaMsTita'],\n",
      " [u'aTa', u'atas', u'katiDApuruzIya', u'SArIra', u'vyAKyA'],\n",
      " [u'aNgAraka', u'iti', u'KyAti', u'gam', u'DarAtmaja'],\n",
      " [u'devaloka', u'ca', u'tvad', u'rUpa', u'BU'],\n",
      " [u'yad', u'ca', u'tvad', u'pUjay', u'caturTI', u'tvad', u'nara'],\n",
      " [u'rUpa', u'tad', u'BU'],\n",
      " [u'evam', u'SAnti', u'kAma']]\n"
     ]
    }
   ],
   "source": [
    "# Look at first 10 sentence roots\n",
    "sentences = DCSSentences(data_file)\n",
    "pprint.pprint(list(itertools.islice(sentences, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can use the hierarchical softmax strategy if we want to be able to score sentences using roots later\n",
    "# The default word2vec model does not use it.\n",
    "use_hier_softmax = False\n",
    "\n",
    "# Prepare keyword args\n",
    "workers = 4        # number of threads to use\n",
    "iterations = 10    # number of iterations (epochs) over the data\n",
    "\n",
    "kwargs = dict(workers=workers, iter=iterations)\n",
    "\n",
    "if use_hier_softmax:\n",
    "    save_filename = \"model_sent_roots_hs.dat\"\n",
    "    kwargs[\"hs\"] = 1\n",
    "    kwargs[\"negative\"] = 0\n",
    "else:\n",
    "    save_filename = \"model_sent_roots.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-06 23:56:27,819 : INFO : collecting all words and their counts\n",
      "2017-08-06 23:56:27,819 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-08-06 23:56:27,989 : INFO : PROGRESS: at sentence #10000, processed 65098 words, keeping 10218 word types\n",
      "2017-08-06 23:56:28,121 : INFO : PROGRESS: at sentence #20000, processed 130561 words, keeping 13805 word types\n",
      "2017-08-06 23:56:28,253 : INFO : PROGRESS: at sentence #30000, processed 194878 words, keeping 16185 word types\n",
      "2017-08-06 23:56:28,385 : INFO : PROGRESS: at sentence #40000, processed 256745 words, keeping 18054 word types\n",
      "2017-08-06 23:56:28,530 : INFO : PROGRESS: at sentence #50000, processed 318673 words, keeping 20161 word types\n",
      "2017-08-06 23:56:28,684 : INFO : PROGRESS: at sentence #60000, processed 387302 words, keeping 22092 word types\n",
      "2017-08-06 23:56:28,805 : INFO : PROGRESS: at sentence #70000, processed 444988 words, keeping 23405 word types\n",
      "2017-08-06 23:56:28,930 : INFO : PROGRESS: at sentence #80000, processed 505208 words, keeping 25474 word types\n",
      "2017-08-06 23:56:29,072 : INFO : PROGRESS: at sentence #90000, processed 575234 words, keeping 27816 word types\n",
      "2017-08-06 23:56:29,213 : INFO : PROGRESS: at sentence #100000, processed 646656 words, keeping 29389 word types\n",
      "2017-08-06 23:56:29,345 : INFO : PROGRESS: at sentence #110000, processed 702190 words, keeping 30960 word types\n",
      "2017-08-06 23:56:29,484 : INFO : PROGRESS: at sentence #120000, processed 748282 words, keeping 32097 word types\n",
      "2017-08-06 23:56:29,720 : INFO : PROGRESS: at sentence #130000, processed 800345 words, keeping 33483 word types\n",
      "2017-08-06 23:56:29,880 : INFO : PROGRESS: at sentence #140000, processed 855770 words, keeping 34485 word types\n",
      "2017-08-06 23:56:30,009 : INFO : PROGRESS: at sentence #150000, processed 916338 words, keeping 35820 word types\n",
      "2017-08-06 23:56:30,147 : INFO : PROGRESS: at sentence #160000, processed 976380 words, keeping 36651 word types\n",
      "2017-08-06 23:56:30,280 : INFO : PROGRESS: at sentence #170000, processed 1035395 words, keeping 37908 word types\n",
      "2017-08-06 23:56:30,417 : INFO : PROGRESS: at sentence #180000, processed 1084207 words, keeping 38711 word types\n",
      "2017-08-06 23:56:30,558 : INFO : PROGRESS: at sentence #190000, processed 1145521 words, keeping 39665 word types\n",
      "2017-08-06 23:56:30,690 : INFO : PROGRESS: at sentence #200000, processed 1218139 words, keeping 41310 word types\n",
      "2017-08-06 23:56:30,815 : INFO : PROGRESS: at sentence #210000, processed 1282518 words, keeping 42312 word types\n",
      "2017-08-06 23:56:30,956 : INFO : PROGRESS: at sentence #220000, processed 1347045 words, keeping 43351 word types\n",
      "2017-08-06 23:56:31,131 : INFO : PROGRESS: at sentence #230000, processed 1414342 words, keeping 44301 word types\n",
      "2017-08-06 23:56:31,326 : INFO : PROGRESS: at sentence #240000, processed 1482694 words, keeping 45094 word types\n",
      "2017-08-06 23:56:31,470 : INFO : PROGRESS: at sentence #250000, processed 1542122 words, keeping 45688 word types\n",
      "2017-08-06 23:56:31,588 : INFO : PROGRESS: at sentence #260000, processed 1597123 words, keeping 46304 word types\n",
      "2017-08-06 23:56:31,696 : INFO : PROGRESS: at sentence #270000, processed 1650510 words, keeping 46785 word types\n",
      "2017-08-06 23:56:31,805 : INFO : PROGRESS: at sentence #280000, processed 1708300 words, keeping 47252 word types\n",
      "2017-08-06 23:56:31,921 : INFO : PROGRESS: at sentence #290000, processed 1774391 words, keeping 48737 word types\n",
      "2017-08-06 23:56:32,046 : INFO : PROGRESS: at sentence #300000, processed 1850536 words, keeping 50540 word types\n",
      "2017-08-06 23:56:32,161 : INFO : PROGRESS: at sentence #310000, processed 1919194 words, keeping 51335 word types\n",
      "2017-08-06 23:56:32,276 : INFO : PROGRESS: at sentence #320000, processed 1988959 words, keeping 51992 word types\n",
      "2017-08-06 23:56:32,391 : INFO : PROGRESS: at sentence #330000, processed 2058045 words, keeping 52663 word types\n",
      "2017-08-06 23:56:32,507 : INFO : PROGRESS: at sentence #340000, processed 2121565 words, keeping 53030 word types\n",
      "2017-08-06 23:56:32,630 : INFO : PROGRESS: at sentence #350000, processed 2187275 words, keeping 53405 word types\n",
      "2017-08-06 23:56:32,740 : INFO : PROGRESS: at sentence #360000, processed 2252584 words, keeping 53691 word types\n",
      "2017-08-06 23:56:32,855 : INFO : PROGRESS: at sentence #370000, processed 2317225 words, keeping 54258 word types\n",
      "2017-08-06 23:56:32,967 : INFO : PROGRESS: at sentence #380000, processed 2380165 words, keeping 55747 word types\n",
      "2017-08-06 23:56:33,092 : INFO : PROGRESS: at sentence #390000, processed 2440963 words, keeping 56445 word types\n",
      "2017-08-06 23:56:33,198 : INFO : PROGRESS: at sentence #400000, processed 2489244 words, keeping 56719 word types\n",
      "2017-08-06 23:56:33,372 : INFO : PROGRESS: at sentence #410000, processed 2557051 words, keeping 57125 word types\n",
      "2017-08-06 23:56:33,528 : INFO : PROGRESS: at sentence #420000, processed 2616067 words, keeping 57908 word types\n",
      "2017-08-06 23:56:33,640 : INFO : PROGRESS: at sentence #430000, processed 2676804 words, keeping 59308 word types\n",
      "2017-08-06 23:56:33,753 : INFO : PROGRESS: at sentence #440000, processed 2744641 words, keeping 60215 word types\n",
      "2017-08-06 23:56:33,872 : INFO : PROGRESS: at sentence #450000, processed 2818267 words, keeping 61223 word types\n",
      "2017-08-06 23:56:33,977 : INFO : PROGRESS: at sentence #460000, processed 2875951 words, keeping 61609 word types\n",
      "2017-08-06 23:56:34,085 : INFO : PROGRESS: at sentence #470000, processed 2933920 words, keeping 61928 word types\n",
      "2017-08-06 23:56:34,203 : INFO : PROGRESS: at sentence #480000, processed 2999440 words, keeping 62788 word types\n",
      "2017-08-06 23:56:34,326 : INFO : PROGRESS: at sentence #490000, processed 3070221 words, keeping 63451 word types\n",
      "2017-08-06 23:56:34,431 : INFO : PROGRESS: at sentence #500000, processed 3133294 words, keeping 63798 word types\n",
      "2017-08-06 23:56:34,569 : INFO : PROGRESS: at sentence #510000, processed 3194117 words, keeping 64891 word types\n",
      "2017-08-06 23:56:34,700 : INFO : PROGRESS: at sentence #520000, processed 3255632 words, keeping 65903 word types\n",
      "2017-08-06 23:56:34,822 : INFO : PROGRESS: at sentence #530000, processed 3311586 words, keeping 66869 word types\n",
      "2017-08-06 23:56:34,951 : INFO : PROGRESS: at sentence #540000, processed 3370605 words, keeping 68975 word types\n",
      "2017-08-06 23:56:35,062 : INFO : PROGRESS: at sentence #550000, processed 3425293 words, keeping 69934 word types\n",
      "2017-08-06 23:56:35,153 : INFO : collected 70640 word types from a corpus of 3465049 raw words and 557484 sentences\n",
      "2017-08-06 23:56:35,154 : INFO : Loading a fresh vocabulary\n",
      "2017-08-06 23:56:35,295 : INFO : min_count=5 retains 22086 unique words (31% of original 70640, drops 48554)\n",
      "2017-08-06 23:56:35,296 : INFO : min_count=5 leaves 3387802 word corpus (97% of original 3465049, drops 77247)\n",
      "2017-08-06 23:56:35,378 : INFO : deleting the raw counts dictionary of 70640 items\n",
      "2017-08-06 23:56:35,384 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2017-08-06 23:56:35,385 : INFO : downsampling leaves estimated 2897021 word corpus (85.5% of prior 3387802)\n",
      "2017-08-06 23:56:35,388 : INFO : estimated required memory for 22086 words and 100 dimensions: 28711800 bytes\n",
      "2017-08-06 23:56:35,477 : INFO : resetting layer weights\n",
      "2017-08-06 23:56:35,829 : INFO : training model with 4 workers on 22086 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-08-06 23:56:36,838 : INFO : PROGRESS: at 1.05% examples, 314341 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:37,865 : INFO : PROGRESS: at 2.04% examples, 295008 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:38,869 : INFO : PROGRESS: at 3.06% examples, 287225 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:39,891 : INFO : PROGRESS: at 4.11% examples, 291581 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:40,894 : INFO : PROGRESS: at 5.26% examples, 297723 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:41,894 : INFO : PROGRESS: at 6.28% examples, 302624 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:42,905 : INFO : PROGRESS: at 7.28% examples, 299577 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:43,908 : INFO : PROGRESS: at 8.10% examples, 293135 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:44,917 : INFO : PROGRESS: at 9.11% examples, 293102 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:45,941 : INFO : PROGRESS: at 10.04% examples, 287720 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:46,947 : INFO : PROGRESS: at 10.79% examples, 281889 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:47,954 : INFO : PROGRESS: at 11.63% examples, 278998 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:48,983 : INFO : PROGRESS: at 12.68% examples, 278391 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:49,999 : INFO : PROGRESS: at 13.50% examples, 274630 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:51,003 : INFO : PROGRESS: at 14.22% examples, 271366 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:52,072 : INFO : PROGRESS: at 14.88% examples, 264267 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:53,072 : INFO : PROGRESS: at 15.57% examples, 261414 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:54,094 : INFO : PROGRESS: at 16.43% examples, 261618 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:55,096 : INFO : PROGRESS: at 17.50% examples, 263552 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:56,121 : INFO : PROGRESS: at 18.52% examples, 264950 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:57,151 : INFO : PROGRESS: at 19.51% examples, 265955 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:58,171 : INFO : PROGRESS: at 20.57% examples, 267110 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:56:59,174 : INFO : PROGRESS: at 21.60% examples, 268419 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:00,201 : INFO : PROGRESS: at 22.73% examples, 269600 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:01,226 : INFO : PROGRESS: at 23.72% examples, 270070 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:02,240 : INFO : PROGRESS: at 24.86% examples, 272019 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:03,275 : INFO : PROGRESS: at 25.89% examples, 273571 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:04,296 : INFO : PROGRESS: at 26.98% examples, 275303 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:05,298 : INFO : PROGRESS: at 28.01% examples, 276077 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:06,313 : INFO : PROGRESS: at 29.07% examples, 277089 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:07,345 : INFO : PROGRESS: at 30.17% examples, 277415 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:08,358 : INFO : PROGRESS: at 31.21% examples, 278227 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:09,378 : INFO : PROGRESS: at 32.28% examples, 278493 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:10,407 : INFO : PROGRESS: at 33.28% examples, 278045 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:11,434 : INFO : PROGRESS: at 34.24% examples, 278552 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:12,460 : INFO : PROGRESS: at 35.36% examples, 279341 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:13,463 : INFO : PROGRESS: at 36.45% examples, 281068 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:14,476 : INFO : PROGRESS: at 37.52% examples, 281452 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:15,479 : INFO : PROGRESS: at 38.61% examples, 282469 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:16,480 : INFO : PROGRESS: at 39.62% examples, 282745 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:17,496 : INFO : PROGRESS: at 40.71% examples, 283217 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:18,529 : INFO : PROGRESS: at 41.71% examples, 283371 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:19,555 : INFO : PROGRESS: at 42.80% examples, 283307 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:20,585 : INFO : PROGRESS: at 43.85% examples, 283563 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:21,608 : INFO : PROGRESS: at 44.98% examples, 284156 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:22,614 : INFO : PROGRESS: at 45.93% examples, 284626 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:23,621 : INFO : PROGRESS: at 47.04% examples, 285483 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:24,644 : INFO : PROGRESS: at 48.06% examples, 285624 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:25,655 : INFO : PROGRESS: at 49.00% examples, 285404 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:26,657 : INFO : PROGRESS: at 50.02% examples, 285118 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:27,658 : INFO : PROGRESS: at 50.80% examples, 284098 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:28,684 : INFO : PROGRESS: at 51.72% examples, 283770 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:29,703 : INFO : PROGRESS: at 52.73% examples, 283278 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:30,721 : INFO : PROGRESS: at 53.78% examples, 283559 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:31,740 : INFO : PROGRESS: at 54.86% examples, 283916 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:32,762 : INFO : PROGRESS: at 55.89% examples, 284517 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:33,789 : INFO : PROGRESS: at 56.94% examples, 285008 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:34,789 : INFO : PROGRESS: at 58.01% examples, 285377 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:35,809 : INFO : PROGRESS: at 59.10% examples, 285860 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:36,834 : INFO : PROGRESS: at 60.20% examples, 285909 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:37,848 : INFO : PROGRESS: at 61.21% examples, 286052 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:38,887 : INFO : PROGRESS: at 62.30% examples, 286120 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:39,904 : INFO : PROGRESS: at 63.37% examples, 286087 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:40,907 : INFO : PROGRESS: at 64.45% examples, 286839 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:41,910 : INFO : PROGRESS: at 65.48% examples, 286993 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:42,927 : INFO : PROGRESS: at 66.59% examples, 287785 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:43,934 : INFO : PROGRESS: at 67.69% examples, 288061 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:44,957 : INFO : PROGRESS: at 68.79% examples, 288614 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:45,983 : INFO : PROGRESS: at 69.81% examples, 288416 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:46,993 : INFO : PROGRESS: at 70.91% examples, 288751 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:48,013 : INFO : PROGRESS: at 71.91% examples, 288834 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:49,029 : INFO : PROGRESS: at 73.04% examples, 288893 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:50,043 : INFO : PROGRESS: at 74.09% examples, 289127 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:51,062 : INFO : PROGRESS: at 75.22% examples, 289401 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:52,069 : INFO : PROGRESS: at 76.16% examples, 289541 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:53,069 : INFO : PROGRESS: at 77.29% examples, 290020 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:54,079 : INFO : PROGRESS: at 78.36% examples, 290297 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:55,101 : INFO : PROGRESS: at 79.48% examples, 290685 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:56,111 : INFO : PROGRESS: at 80.52% examples, 290626 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:57,114 : INFO : PROGRESS: at 81.62% examples, 291024 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:58,135 : INFO : PROGRESS: at 82.69% examples, 290901 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:57:59,140 : INFO : PROGRESS: at 83.80% examples, 291247 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:00,161 : INFO : PROGRESS: at 84.90% examples, 291385 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:01,190 : INFO : PROGRESS: at 85.94% examples, 291767 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:02,207 : INFO : PROGRESS: at 87.02% examples, 292032 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:03,230 : INFO : PROGRESS: at 88.15% examples, 292418 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:04,249 : INFO : PROGRESS: at 89.24% examples, 292671 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:05,260 : INFO : PROGRESS: at 90.33% examples, 292671 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:06,280 : INFO : PROGRESS: at 91.36% examples, 292673 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:07,296 : INFO : PROGRESS: at 92.47% examples, 292722 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:08,319 : INFO : PROGRESS: at 93.47% examples, 292528 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:09,349 : INFO : PROGRESS: at 94.58% examples, 292886 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:10,368 : INFO : PROGRESS: at 95.61% examples, 292962 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:11,378 : INFO : PROGRESS: at 96.61% examples, 293132 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:12,444 : INFO : PROGRESS: at 97.75% examples, 293187 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:13,460 : INFO : PROGRESS: at 98.81% examples, 293465 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:14,484 : INFO : PROGRESS: at 99.91% examples, 293441 words/s, in_qsize 0, out_qsize 0\n",
      "2017-08-06 23:58:14,552 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-08-06 23:58:14,553 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-08-06 23:58:14,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-08-06 23:58:14,562 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-08-06 23:58:14,565 : INFO : training on 34650490 raw words (28969191 effective words) took 98.7s, 293418 effective words/s\n",
      "2017-08-06 23:58:14,566 : INFO : saving KeyedVectors object under model_sent_roots.dat, separately None\n",
      "2017-08-06 23:58:14,568 : INFO : not storing attribute syn0norm\n",
      "2017-08-06 23:58:14,859 : INFO : saved model_sent_roots.dat\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sentences, **kwargs)\n",
    "# Since we will not be doing any more training, we switch over to the KeyedVectors that are generated and only save those\n",
    "model = model.wv\n",
    "model.save(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-06 23:58:54,618 : INFO : loading KeyedVectors object from model_sent_roots.dat\n",
      "2017-08-06 23:58:54,756 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-08-06 23:58:54,756 : INFO : loaded model_sent_roots.dat\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.KeyedVectors.load(save_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some (interesting?) relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convenience function for devanagari output\n",
    "def print_similar_devanagari(word_slp):\n",
    "    s = sanscript.transliterate(word_slp, sanscript.SLP1, sanscript.DEVANAGARI) + \" -- \"\n",
    "    similar = model.wv.most_similar(word_slp)\n",
    "    for result in similar:\n",
    "        s += sanscript.transliterate(result[0], sanscript.SLP1, sanscript.DEVANAGARI) + u\" ({:0.4f}) | \".format(result[1])\n",
    "    print(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model appears to have correctly learned some synonyms/common contexts for brAhmaNa, dharma, idAnIm, etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-06 23:59:35,224 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ब्राह्मण -- द्विजाति (0.6738) | द्विज (0.6690) | विप्र (0.6587) | अतिथि (0.6003) | श्रोत्रिय (0.5574) | शूद्र (0.5392) | द्विजोत्तम (0.5375) | याजक (0.5345) | वैश्य (0.5314) | ऋत्विज् (0.5281) | \n",
      "\n",
      "धर्म -- स्वधर्म (0.6565) | धर्म्य (0.6315) | आचार (0.5989) | अधर्म (0.5866) | कृतात्मन् (0.5559) | स्मार्त (0.5440) | व्यवसाय (0.5299) | धार्मिक (0.5162) | नय (0.5116) | वृत्ति (0.5096) | \n",
      "\n",
      "इदानीम् -- अधुना (0.6937) | किमर्थ (0.6628) | सांप्रतम् (0.6600) | अवश्यम् (0.6560) | हन्त (0.6541) | सम्प्रति (0.6315) | विवक्ष् (0.6172) | स्वामिन् (0.5934) | कस्मात् (0.5916) | यथातथ (0.5904) | \n",
      "\n",
      "पुत्र -- सुत (0.8519) | तनय (0.7341) | आत्मज (0.6885) | अपत्य (0.6275) | दायाद (0.6238) | सूनु (0.5822) | स्नुषा (0.5657) | स्याल (0.5491) | भार्या (0.5377) | सुता (0.5363) | \n",
      "\n",
      "पत्नी -- भार्या (0.7047) | जाया (0.6891) | अरुन्धती (0.6738) | सती (0.6578) | दाक्षायणी (0.6513) | दुहितृ (0.6300) | सुता (0.6262) | मेना (0.6261) | स्वसृ (0.6036) | देवर (0.5940) | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [\"brAhmaRa\", \"Darma\", \"idAnIm\", \"putra\", \"patnI\"]\n",
    "for word in words:\n",
    "    print_similar_devanagari(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "श्रु -- निशामय् (0.6804) | निबुध् (0.6460) | आकर्णय् (0.6092) | संश्रु (0.6029) | कथय् (0.5691) | उपश्रु (0.5514) | श्रावय् (0.5473) | कीर्तय् (0.5161) | समाचक्ष् (0.5140) | कथ् (0.5139) | \n",
      "\n",
      "वच् -- अभिधा (0.7199) | अह् (0.7132) | ब्रू (0.7086) | भाष् (0.7014) | प्रतिवच् (0.6943) | प्राह् (0.6919) | विज्ञापय् (0.6333) | व्याहृ (0.6307) | प्रवच् (0.6287) | आचक्ष् (0.6264) | \n",
      "\n",
      "गम् -- प्रया (0.7556) | व्रज् (0.7523) | या (0.7329) | नी (0.6761) | आगम् (0.6707) | प्रस्था (0.6436) | उपागम् (0.6378) | उपया (0.6370) | प्रतिगम् (0.6304) | उपगम् (0.6289) | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "roots = [\"Sru\", \"vac\", \"gam\"]\n",
    "for root in roots:\n",
    "    print_similar_devanagari(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at some characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "युधिष्ठिर -- सुयोधन (0.6517) | धर्मसुत (0.6478) | पाण्डव (0.6346) | धर्मराज (0.6299) | धनंजय (0.6193) | महीपति (0.6065) | वृकोदर (0.6054) | दुर्योधन (0.5988) | धर्मपुत्र (0.5983) | अजातशत्रु (0.5955) | \n",
      "\n",
      "भीम -- भीमसेन (0.7958) | वृकोदर (0.6609) | किरीटिन् (0.6493) | सूतपुत्र (0.6425) | भैमसेनि (0.6417) | सात्यकि (0.6379) | मारुति (0.6364) | राधेय (0.6282) | युयुधान (0.6266) | प्रहस्त (0.6250) | \n",
      "\n",
      "अर्जुन -- फल्गुन (0.8275) | धनंजय (0.7889) | बीभत्सु (0.7321) | सात्यकि (0.7128) | सव्यसाचिन् (0.7106) | पार्थ (0.6977) | राधेय (0.6902) | पाण्डव (0.6768) | गाण्डीवधन्वन् (0.6687) | वृकोदर (0.6624) | \n",
      "\n",
      "कृष्ण -- शुक्ल (0.5317) | पीत (0.5152) | वासुदेव (0.5089) | गोविन्द (0.4953) | जनार्दन (0.4815) | पार्थ (0.4739) | केशव (0.4666) | चतुर्दशी (0.4662) | श्वेत (0.4648) | हृषीकेश (0.4527) | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "characters = [\"yuDizWira\", \"BIma\", \"arjuna\", \"kfzRa\"]\n",
    "for c in characters:\n",
    "    print_similar_devanagari(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting (ironic?) that the model thinks duryodhana/suyodhana is mentioned in similar contexts to yudhiShThira. For bhIma and arjuna, we see karNa show up in the similar words, and some synonyms are correctly learned.\n",
    "For kRShNa, the embedding learns synonyms of the character kRSNa, along with the antonyms of the meaning of kRShNa = black. \n",
    "\n",
    "**Some other examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्रवाल -- विद्रुम (0.8978) | मुक्ताफल (0.8927) | वैडूर्य (0.8830) | मरकत (0.8827) | मुक्ता (0.8776) | कुट्टिम (0.8745) | मौक्तिक (0.8674) | इन्द्रनील (0.8508) | पद्मराग (0.8479) | पुष्पराग (0.8462) | \n",
      "\n",
      "चतुर् -- त्रि (0.6457) | अष्टन् (0.6245) | षष् (0.6100) | द्वि (0.5971) | पञ्चन् (0.5778) | नवन् (0.5525) | एकैक (0.5377) | द्वादशन् (0.5293) | षोडशन् (0.5104) | पञ्चदशन् (0.5022) | \n",
      "\n",
      "कवि -- काव्य (0.7199) | अविनाशिन् (0.6559) | सूरि (0.6375) | मन्तृ (0.6307) | आयुर्वेद (0.6290) | वेत्तृ (0.6277) | व्याकरण (0.6223) | उपनिषद् (0.6197) | विपरिलोप (0.6098) | नाट्य (0.6096) | \n",
      "\n",
      "भूपाल -- महीपाल (0.6893) | वीरवर (0.6221) | अम्बा (0.6214) | द्वाःस्थ (0.6097) | गुह (0.6016) | धर्मसूनु (0.6003) | वसुधाधिप (0.5971) | रघुनन्दन (0.5923) | दर्पसार (0.5903) | शैलेन्द्र (0.5896) | \n",
      "\n",
      "शाल्मली -- शाल्मलि (0.9005) | शिरीष (0.8990) | पीलु (0.8976) | फलिनी (0.8938) | वेतस (0.8923) | उशीर (0.8914) | शेलु (0.8875) | धातकी (0.8875) | कोविदार (0.8869) | इङ्गुद (0.8860) | \n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = [\"pravAla\", \"catur\", \"kavi\", \"BUpAla\", \"SAlmalI\"]\n",
    "for word in words:\n",
    "    print_similar_devanagari(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "For a first pass, the above results are surprisingly good, and pretty interesting. They seem to confirm that it might be possible to use embeddings using just roots (prAtipadika, dhAtus) as a building block for deep learning. It might be interesting to see what happens once we add in the vibhakti/vacana/puruSha/lakAra/etc. tags. Perhaps it would make sense to learn a separate embedding for them, or just combine all the tags and root words? More experiments are certainly needed ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
